# -*- coding: utf-8 -*-
"""RLfB_CW1_submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ztjQPYz85mmb-NoSqqFm_ej77yyF3av
"""

def get_CID():
  return "01739903"

def get_login():
  return "jb1521"

# This class define the Dynamic Programing agent

class DP_agent(object):

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script
  def solve(self, env):
    """
    Solve a given Maze environment using Dynamic Programming
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - V {np.array} -- Corresponding value function
    """

    # Initialisation (can be edited)
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    V = np.zeros(env.get_state_size())

    ####
    # Add your code here
    # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
    ####

    # Undergoing value iteration
    threshold = 1e-3 # break end
    delta = threshold

    while delta >= threshold:
      delta = 0

      # Looping over every state
      for prior_state in range(env.get_state_size()):

        # Self-absorbing state check
        if not env.get_absorbing()[0, prior_state]:

          # Storing previous value for state
          v = V[prior_state]

          # Computing Q for state
          Q = np.zeros(4)
          for post_state in range(env.get_state_size()):
            Q += env.get_T()[prior_state, post_state, :] * (env.get_R()[prior_state, post_state, :] + env.get_gamma() * V[post_state])

          V[prior_state] = np.max(Q)

          delta = max(delta, np.abs(v - V[prior_state]))

    # Finding optimal policy for successful value iteration
    for prior_state in range(env.get_state_size()):

      Q = np.zeros(4)
      for post_state in range(env.get_state_size()):
        Q += env.get_T()[prior_state, post_state, :] * (env.get_R()[prior_state, post_state, :] + env.get_gamma() * V[post_state])

      #Selecting optimal action (policy) for each state
      policy[prior_state, np.argmax(Q)] = 1

    return policy, V

#### Monte-Carlo Agent for Question 2 ####

def epsilon_action(a, eps=0.1):
  p = np.random.random()
  if p > eps:
    return a
  else:
    return np.random.randint(0,4)

class MC_agent(object):

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script
  def solve(self, env):
    """
    Solve a given Maze environment using Monte Carlo learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """

    # Initialisation (can be edited)
    Q = np.zeros((env.get_state_size(), env.get_action_size()))
    V = np.zeros(env.get_state_size())
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    values = [V]
    total_rewards = []

    # setting random policy
    for s in range(env.get_state_size()):
      num = np.random.randint(0,4)
      policy[s, num] = 1

    # initialising returns
    returns = {}
    for s in range(env.get_state_size()):
      for a in range(env.get_action_size()):
        returns[(s,a)] = []

    # total episodes
    N = 10000
    # initial episodes
    n = 0
    # exploration vs exploitation parameter
    epsilon = 0.4

    # actions array according to policy
    policy_actions = [np.argmax(s) for s in policy]


    ### STARTING SEQUENCE ###

    while n < N:

      # resetting maze to get access to initial values
      t, state, reward, done = env.reset()

      # total reward for episode counter
      episode_reward = 0

      # getting initial action
      action = epsilon_action(policy_actions[state], epsilon)

      # initialising state, action, reward tracker
      sa_rewards = [(state, action, reward)]



      ### GENERATING EPISODE ###

      while done == 0:

        # taking step
        t, state, reward, done = env.step(action)

        # new action
        action = epsilon_action(policy_actions[state], epsilon)

        # adding to state, action, rewards
        sa_rewards.append((state, action, reward))

        episode_reward += reward

      ### END GENERATING EPISODE ###



      ### UPDATING RETURNS AND CALCULATING NEW Q(s,a), POLICY AND VALUE FUNCTION ###
      # creating state, action, return
      sa_returns = []
      G = 0

      # calculating and adding to state, action, return
      for s, a, r in reversed(sa_rewards):
        sa_returns.append((s, a, G))
        G = r + env.get_gamma() * G
      sa_returns.reverse()

      # adding to returns if it is the first pass
      seen_sa_pairs = set()
      for s, a, G in sa_returns:
        sa = (s, a)

        if sa not in seen_sa_pairs:
          returns[sa].append(G)
          Q[s, a] = np.mean(returns[sa])
          seen_sa_pairs.add(sa)

      # setting new policy_actions
      policy_actions = [np.argmax(Q[s]) for s in range(env.get_state_size())]

      # setting new value function
      for s in range(env.get_state_size()):
        # policy[s] = [1 if i == np.argmax(Q[s]) else 0 for i in range(env.get_action_size())]
        V[s] = np.max(Q[s])

      # appending new value function to values
      values.append(V)

      ### END UPDATING ###

      # appending episode_reward to total_rewards
      total_rewards.append(episode_reward)

      # adding to counter
      n += 1

    ### END SEQUENCE ###

    # creating final policy
    for s in range(env.get_state_size()):
      policy[s] = [1 if i == policy_actions[s] else 0 for i in range(env.get_action_size())]

    return policy, values, total_rewards



# #### Monte Carlo Agent for Question 3, for viewing only ####
# class MC_agent(object):

#   def __init__(self, epsilon=0.1, decay_rate=0.99, temperature=1.0):
#     # Initialize parameters for exploration strategies
#     self.epsilon = epsilon  # For epsilon-greedy
#     self.decay_rate = decay_rate  # For decaying epsilon
#     self.temperature = temperature  # For Softmax

#   # Placeholder for epsilon-greedy strategy
#   def epsilon_greedy_action(self, state, q_values, epsilon):
#         # Implement epsilon-greedy selection logic
#         if np.random.rand() < epsilon:
#             return np.random.choice(range(len(q_values[state])))
#         else:
#           return np.argmax(q_values[state])

#   # Placeholder for softmax strategy
#   def softmax_action(self, state, q_values, temperature):
#       # Implement softmax selection logic here
#       preferences = q_values[state] / temperature
#       exp_preferences = np.exp(preferences - np.max(preferences))
#       probabilities = exp_preferences / np.sum(exp_preferences)
#       return np.random.choice(range(len(q_values[state])), p=probabilities)

#   def choose_action(self, state, q_values, strategy='fixed_epsilon', episode=1):
#     # Chooses an action based on the specified exploration strategy.
#     if strategy == 'fixed_epsilon':
#         return self.epsilon_greedy_action(state, q_values, epsilon=self.epsilon)
#     elif strategy == 'decaying_epsilon':
#         # Student should modify decayed_epsilon as per requirement
#         decayed_epsilon = self.epsilon * (self.decay_rate ** episode)
#         return self.epsilon_greedy_action(state, q_values, epsilon=decayed_epsilon)
#     elif strategy == 'softmax':
#         # Implement softmax strategy logic
#         return self.softmax_action(state, q_values, temperature=self.temperature)

#   # [Action required]
#   # WARNING: make sure this function can be called by the auto-marking script
#   def solve(self, env, strategy='fixed_epsilon', learning_episodes=10000, total_episodes=10000):
#     """
#     Solve a given Maze environment using Monte Carlo learning
#     input: env {Maze object} -- Maze to solve
#     output:
#       - policy {np.array} -- Optimal policy found to solve the given Maze environment
#       - values {list of np.array} -- List of successive value functions for each episode
#       - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
#     """
#     # Initialisation (can be edited)
#     Q = np.zeros((env.get_state_size(), env.get_action_size()))
#     V = np.zeros(env.get_state_size())
#     policy = np.zeros((env.get_state_size(), env.get_action_size()))
#     values = [V]
#     total_rewards = []

#     # setting random policy
#     for s in range(env.get_state_size()):
#       num = np.random.randint(0,4)
#       policy[s, num] = 1

#     # initialising returns
#     returns = {}
#     for s in range(env.get_state_size()):
#       for a in range(env.get_action_size()):
#         returns[(s,a)] = []

#     # initial episodes
#     n = 0
#     # exploration vs exploitation parameter
#     epsilon = 0.4

#     # actions array according to policy
#     policy_actions = [np.argmax(s) for s in policy]


#     ### STARTING SEQUENCE ###

#     while n < total_episodes:

#       # resetting maze to get access to initial values
#       t, state, reward, done = env.reset()

#       # total reward for episode counter
#       episode_reward = 0

#       # getting initial action
#       action = epsilon_action(policy_actions[state], epsilon)

#       # initialising state, action, reward tracker
#       sa_rewards = [(state, action, reward)]



#       ### GENERATING EPISODE ###

#       while done == 0:

#         # taking step
#         t, state, reward, done = env.step(action)

#         # new action
#         action = epsilon_action(policy_actions[state], epsilon)

#         # adding to state, action, rewards
#         sa_rewards.append((state, action, reward))

#         episode_reward += reward

#       ### END GENERATING EPISODE ###


#       if n < learning_episodes:
#           ### UPDATING RETURNS AND CALCULATING NEW Q(s,a), POLICY AND VALUE FUNCTION ###
#           # creating state, action, return
#           sa_returns = []
#           G = 0

#           # calculating and adding to state, action, return
#           for s, a, r in reversed(sa_rewards):
#             sa_returns.append((s, a, G))
#             G = r + env.get_gamma() * G
#           sa_returns.reverse()

#           # adding to returns if it is the first pass
#           seen_sa_pairs = set()
#           for s, a, G in sa_returns:
#             sa = (s, a)

#             if sa not in seen_sa_pairs:
#               returns[sa].append(G)
#               Q[s, a] = np.mean(returns[sa])
#               seen_sa_pairs.add(sa)

#           # setting new policy_actions
#           policy_actions = [np.argmax(Q[s]) for s in range(env.get_state_size())]

#           # setting new value function
#           for s in range(env.get_state_size()):
#             # policy[s] = [1 if i == np.argmax(Q[s]) else 0 for i in range(env.get_action_size())]
#             V[s] = np.max(Q[s])

#           # appending new value function to values
#           values.append(V)

#       ### END UPDATING ###

#       # appending episode_reward to total_rewards
#       total_rewards.append(episode_reward)

#       # adding to counter
#       n += 1

#     ### END SEQUENCE ###

#     # creating final policy
#     for s in range(env.get_state_size()):
#       policy[s] = [1 if i == policy_actions[s] else 0 for i in range(env.get_action_size())]

#     return policy, values, total_rewards