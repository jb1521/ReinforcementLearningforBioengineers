{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRjkr9rnRmznn/MzFDnyUh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "97VJA7jO48Oo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z6qM4Uy4rY1"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[classic-control]\n",
        "!pip install -q mujoco\n",
        "!pip install -q mujoco-py\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "import collections, random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "lx_q6cPn4-Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pi           = 0.0005 # policy learning rate\n",
        "lr_q            = 0.001 # Q-function learning rate\n",
        "init_alpha      = 0.01 # trade-off coefficient\n",
        "gamma           = 0.98 # discount factor\n",
        "batch_size      = 32\n",
        "buffer_limit    = 50000\n",
        "tau             = 0.01 # for target network soft update\n",
        "target_entropy  = -1.0 # for automated alpha update\n",
        "lr_alpha        = 0.001  # for automated alpha update"
      ],
      "metadata": {
        "id": "oq91DfNN40ld"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replay Buffer"
      ],
      "metadata": {
        "id": "yZzrgVd45yzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "    # initialising a double-ended queue to establish the replay buffer\n",
        "    def __init__(self):\n",
        "      self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    # appending a transition to the replay buffer\n",
        "    def put(self, transition):\n",
        "      self.buffer.append(transition)\n",
        "\n",
        "    # taking a random sample (size n) from the replay buffer to train the model\n",
        "    def sample(self, n):\n",
        "      mini_batch = random.sample(self.buffer, n) # batch from replay buffer\n",
        "      s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], [] # lists for items in batch\n",
        "\n",
        "      # adding items of a transition to respective list\n",
        "      for transition in mini_batch:\n",
        "        s, a, r, s_prime, done = transition\n",
        "        s_lst.append(s)\n",
        "        a_lst.append([a])\n",
        "        r_lst.append([r])\n",
        "        s_prime_lst.append(s_prime)\n",
        "        done_mask = 0.0 if done else 1.0\n",
        "        done_mask_lst.append([done_mask])\n",
        "\n",
        "      # returing the lists as tensors\n",
        "      return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.float), \\\n",
        "                torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "                torch.tensor(done_mask_lst, dtype=torch.float)"
      ],
      "metadata": {
        "id": "H89pGh5y50jy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Network"
      ],
      "metadata": {
        "id": "PLpLIcDh7D-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    # initialises a neural network from nn.Module\n",
        "    def __init__(self, learning_rate):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 128) # fully-connected layer, with 3 input neurons and 128 output neurons\n",
        "        self.fc_mu = nn.Linear(128,1) # mean averaging, 128 inputs give one scalar\n",
        "        self.fc_std  = nn.Linear(128,1) # standard deviation, 128 inputs give one scalar\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate) # optimise parameters of the model\n",
        "\n",
        "        self.log_alpha = torch.tensor(np.log(init_alpha)) # logarithm of entropy regularisation factor\n",
        "        self.log_alpha.requires_grad = True # enables gradient of log_alpha\n",
        "        self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=lr_alpha) # optimise log_alpha through training\n",
        "\n",
        "    # forward pass of neural network\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # input processing\n",
        "        mu = self.fc_mu(x)\n",
        "        std = F.softplus(self.fc_std(x))\n",
        "        dist = Normal(mu, std) # action distribution\n",
        "        action = dist.rsample() # action sampling\n",
        "        log_prob = dist.log_prob(action)\n",
        "        real_action = torch.tanh(action) # action transformation, limits between [-1,1]\n",
        "        real_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7) # log probablity adjustment\n",
        "        return real_action, real_log_prob\n",
        "\n",
        "    # training method\n",
        "    def train_net(self, q1, q2, mini_batch):\n",
        "        s, _, _, _, _ = mini_batch\n",
        "        a, log_prob = self.forward(s) # policy output\n",
        "        entropy = -self.log_alpha.exp() * log_prob # entropy regularisation\n",
        "\n",
        "        # critic networks\n",
        "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
        "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
        "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
        "\n",
        "        #loss calculation\n",
        "        loss = -min_q - entropy # for gradient ascent\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # entropy regularisation factor update\n",
        "        self.log_alpha_optimizer.zero_grad()\n",
        "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
        "        alpha_loss.backward()\n",
        "        self.log_alpha_optimizer.step()"
      ],
      "metadata": {
        "id": "beav-bFf7Gst"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-Function Network"
      ],
      "metadata": {
        "id": "6cZcAUYS-UBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNet(nn.Module):\n",
        "    # initialisation, layers, and optimisation\n",
        "    def __init__(self, learning_rate):\n",
        "        super(QNet, self).__init__()\n",
        "        self.fc_s = nn.Linear(4, 64)\n",
        "        self.fc_a = nn.Linear(1,64)\n",
        "        self.fc_cat = nn.Linear(128,32)\n",
        "        self.fc_out = nn.Linear(32,1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, x, a):\n",
        "        h1 = F.relu(self.fc_s(x))\n",
        "        h2 = F.relu(self.fc_a(a))\n",
        "        cat = torch.cat([h1,h2], dim=1) # feature concatenation\n",
        "        q = F.relu(self.fc_cat(cat)) # Q-value estimation\n",
        "        q = self.fc_out(q)\n",
        "        return q\n",
        "\n",
        "    # training method\n",
        "    def train_net(self, target, mini_batch):\n",
        "        s, a, r, s_prime, done = mini_batch\n",
        "        loss = F.smooth_l1_loss(self.forward(s, a) , target) # loss calculation, 'Huber loss'\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step() # optimisation\n",
        "\n",
        "    # soft update method, implementing soft updates for a target network\n",
        "    def soft_update(self, net_target):\n",
        "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
        "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
      ],
      "metadata": {
        "id": "WxuLJ3kQ-WAa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Target"
      ],
      "metadata": {
        "id": "VXrRigZ5-ZU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target calculation\n",
        "def calc_target(pi, q1, q2, mini_batch):\n",
        "    s, a, r, s_prime, done = mini_batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        a_prime, log_prob= pi(s_prime) # next action\n",
        "        entropy = -pi.log_alpha.exp() * log_prob # entropy regularisation\n",
        "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime) # clipped double-Q\n",
        "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
        "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
        "        target = r + gamma * done * (min_q + entropy) # target Q\n",
        "\n",
        "    return target"
      ],
      "metadata": {
        "id": "-L76R5PE-gWy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Curve Plotting Function"
      ],
      "metadata": {
        "id": "ronEx45_PX5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "main Function"
      ],
      "metadata": {
        "id": "sxjSowdg-zL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    # initialisation\n",
        "    env = gym.make('InvertedPendulum-v5', render_mode='rgb_array')\n",
        "    memory = ReplayBuffer()\n",
        "    q1, q2, q1_target, q2_target = QNet(lr_q), QNet(lr_q), QNet(lr_q), QNet(lr_q)\n",
        "    pi = PolicyNet(lr_pi)\n",
        "\n",
        "    q1_target.load_state_dict(q1.state_dict())\n",
        "    q2_target.load_state_dict(q2.state_dict())\n",
        "\n",
        "    score = 0.0\n",
        "    print_interval = 20\n",
        "\n",
        "    # training loop\n",
        "    for n_epi in range(1000):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        count = 0\n",
        "\n",
        "        # policy interaction\n",
        "        while count < 500 and not done:\n",
        "            a, log_prob= pi(torch.from_numpy(s).float())\n",
        "            step_result = env.step([a.item()])\n",
        "            if len(step_result) == 5:\n",
        "                s_prime, r, done, truncated, info = step_result\n",
        "            else:  # Handle case with 4 return values\n",
        "                s_prime, r, done, info = step_result\n",
        "                truncated = False  # Assume not truncated if missing\n",
        "            memory.put((s, a.item(), r/10.0, s_prime, done)) # dividing r by 10 for reward scaling\n",
        "            score += r\n",
        "            s = s_prime\n",
        "            count += 1\n",
        "\n",
        "        # network training\n",
        "        if memory.size()>1000:\n",
        "            for i in range(20):\n",
        "                mini_batch = memory.sample(batch_size)\n",
        "                td_target = calc_target(pi, q1_target, q2_target, mini_batch)\n",
        "                q1.train_net(td_target, mini_batch)\n",
        "                q2.train_net(td_target, mini_batch)\n",
        "                entropy = pi.train_net(q1, q2, mini_batch)\n",
        "                q1.soft_update(q1_target)\n",
        "                q2.soft_update(q2_target)\n",
        "\n",
        "\n",
        "        # monitoring\n",
        "        if n_epi%print_interval==0 and n_epi!=0:\n",
        "            print(\"# of episode :{}, avg score : {:.1f}, alpha:{:.4f}\".format(n_epi, score/print_interval, pi.log_alpha.exp()))\n",
        "            score = 0.0\n",
        "\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "cceFOpS1-mWj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "8CM4TmYU-6Yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
